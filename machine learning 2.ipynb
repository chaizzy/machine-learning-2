{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648a16ae-d240-44a6-93c0-31c18cef6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1:\n",
    "# \"Overfitting\"\n",
    "# Overfitting happens when a model learns the training data too well and becomes too specific to the training examples, \n",
    "# to the point that it performs poorly on new, unseen data\n",
    "# for eg: Train dataset --- model is trained -- accuracy 95%\n",
    "#          Test dataset ---- model is tested --- accuracy 60 % \n",
    "# cosequences : low bias , high variance\n",
    "# mitigation\n",
    "# 1.Feature selection: Removing irrelevant or redundant features from the model, \n",
    "#   reducing its complexity and the likelihood of overfitting.\n",
    "# 2.Early stopping: Monitoring the model's performance during training and \n",
    "#    stopping the training process when the performance on the validation set starts to deteriorate.\n",
    "\n",
    "\n",
    "# \"underfitting\"\n",
    "# Underfitting occurs when a model is too simple or lacks the capacity to capture the underlying patterns in the training data. \n",
    "#  The model fails to learn important relationships and, as a result, performs poorly both on the training data and new data.\n",
    "# for eg: Train dataset --- model is trained -- accuracy 55%\n",
    "#          Test dataset ---- model is tested --- accuracy 50 % \n",
    "# cosequences : high bias , high variance\n",
    "# mitigation\n",
    "# 1. Feature engineering: Create new features or transformations of existing features that help the model better represent the underlying patterns.\n",
    "# 2. Increase training data: Collect more data to provide the model with a larger and more diverse set of examples, allowing it to learn better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a2b1983-fbdf-481f-9c9b-bf1bd0bead09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2 :\n",
    "# 1.Feature selection: Removing irrelevant or redundant features from the model, \n",
    "#   reducing its complexity and the likelihood of overfitting.\n",
    "# 2.Early stopping: Monitoring the model's performance during training and \n",
    "#    stopping the training process when the performance on the validation set starts to deteriorate.\n",
    "# 3.Cross-validation: Split your data into training and validation sets. \n",
    "#    Use the validation set to evaluate your model's performance and fine-tune its hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd8e4e5-3ef5-4487-b229-da2401623bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3 :\n",
    "# # \"underfitting\"\n",
    "# Underfitting occurs when a model is too simple or lacks the capacity to capture the underlying patterns in the training data. \n",
    "#  The model fails to learn important relationships and, as a result, performs poorly both on the training data and new data.\n",
    "# for eg: Train dataset --- model is trained -- accuracy 55%\n",
    "#          Test dataset ---- model is tested --- accuracy 50 % \n",
    "\n",
    "# scenerios\n",
    "# 1. Limited training data---didn't have suffient data to train\n",
    "# 2. Inappropriate feature selection: If important features are omitted or irrelevant features are included in the model, \n",
    "#      it can lead to underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdcf529-5e57-4e92-b25c-ed712b9be559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4 :\n",
    "# Bias variance tradeoff\n",
    "# The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between bias and variance \n",
    "#and how they impact the performance of a model.\n",
    "\n",
    "# A model with high bias makes strong assumptions about the data, leading to underfitting.\n",
    "#  effects :  it fails to capture the underlying patterns and relationships in the data, resulting in systematic errors and poor predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aee5708-0bd2-475b-bcff-d2bb7f8ccf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5 :\n",
    "# 1.train/Validation/Test Split: Divide your dataset into three subsets: the training set, the validation set, and the test set.\n",
    "#   The training set is used to train the model, the validation set helps in monitoring the model's performance during training, and the test set evaluates the final model.\n",
    "# 2.Cross-validation: Split your data into training and validation sets. \n",
    "#    Use the validation set to evaluate your model's performance and fine-tune its hyperparameters \n",
    "#  3. Performance on Unseen Data: Finally, evaluate your model's performance on the test set or real-world data that was not used during training.\n",
    "\n",
    "# we can determine by looking at the accuracy of trained dataset and the model which is tested "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abcf468c-c4ec-4a97-ad9d-e249d1585bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6 :\n",
    "# Bias\n",
    "# Bias refers to the error introduced by approximating a real-world problem with a simplified model. \n",
    "# It represents the model's tendency to consistently make certain assumptions about the data.\n",
    "# High bias models have limited expressiveness and may oversimplify the underlying patterns in the data. \n",
    "# example :  regression with very few features or a decision tree with limited depth\n",
    "# performance : high bias models typically have low training and validation performance. \n",
    "\n",
    "# variance\n",
    "# Variance refers to the model's sensitivity to fluctuations in the training data. \n",
    "#  It represents the amount by which the model's predictions would change if trained on different subsets of the data.\n",
    "# example : high variance models include deep neural networks with many layers and parameters or decision trees with large depths\n",
    "# performance :  high variance models typically exhibit excellent training performance but poor performance on unseen data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8719e-1be8-4088-9c19-1ae33dd2506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 7 :\n",
    "# \"Regularization\" \n",
    "# it is  a technique used in machine learning to prevent overfitting by adding a penalty term to the model's loss function. \n",
    "\n",
    "# they prevent overfitting by \n",
    "# controling  the trade-off between fitting the training data well and keeping the model's complexity in check.\n",
    "\n",
    "# Techniques\n",
    "# 1. L1 Regularization (Lasso): L1 regularization adds the sum of the absolute values of the model's coefficients as a penalty term to the loss function. \n",
    "# 2. L2 Regularization (Ridge): L2 regularization adds the sum of the squared values of the model's coefficients as a penalty term to the loss function\n",
    "# 3. Elastic Net Regularization: Elastic Net regularization combines L1 and L2 regularization. It adds both the sum of the absolute values and \n",
    "#                                the sum of the squared values of the coefficients to the loss function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
